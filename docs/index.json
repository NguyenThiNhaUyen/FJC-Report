[
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.3-s3-secrets-manager/5.3.1-create-secrets-manager/",
	"title": "Create secrets manager",
	"tags": [],
	"description": "",
	"content": " Open the Amazon Secrets manager console In the navigation pane, choose Secrets, then click Store a new secret: In the Create endpoint console: Choose \u0026lsquo;Other type of secret\u0026rsquo; In Key/value pairs, we will have 5 secret names All the next is default. After we create all 5 secrets manager: "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Thi Nha Uyen\nPhone Number: 0938343715\nEmail: uyenntnse183774@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 24/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "Workshop overview Here is a proposed overview of how AWS services are used to set up the Metropolitano system, ensuring the criteria of \u0026ldquo;high availability - secure - scalable\u0026rdquo;:Overview of Metropolitano System Architecture on AWS\nThe system is designed to manage, monitor and coordinate urban railway operations, using a range of AWS services for each functional layer:\nLayer AWS Services Role on the Metropolitano system Network \u0026amp; Security Route 53 Manage DNS, route user (operator, management) traffic to applications efficiently and reliably. CloudFront Content delivery network (CDN), which helps deliver static content (user interface, reports) with low latency and enhanced security. WAF (Web Application Firewall) Protect web applications from common network attacks, filtering malicious traffic before it reaches the application server. Secrets Manager Securely manage, rotate, and control access to sensitive information such as database (RDS) passwords, API keys, and other credentials. Helps strengthen application security and minimize the risk of information leakage. Data storage \u0026amp; Application S3(Simple Storage Service) Store all unstructured and rarely accessed data, such as logs, backups, and project documents EC2(Elastic Compute Cloud) Provides compute resources to run back-end applications, orchestration services, and user interfaces. Can be used in Auto Scaling Groups to ensure high availability. RDS (Relational Database Service) - MSSQL Provides a fully managed relational database running on the Microsoft SQL Server platform. Used to store structured, mission-critical business data (route information, schedules, equipment status). Collect \u0026amp; process Kinesis Build event-driven architectures, automate workflows by connecting different AWS services when events occur (e.g., when data reaches an alert threshold) EventBridge Build event-driven architectures, automate workflows by connecting different AWS services when events occur (e.g., when data reaches an alert threshold) SQS (Simple Queue Service) Message queues to decouple system components, handle asynchronous tasks (e.g., sending mass notifications, handling dispatch commands). SNS (Simple Notification Service) Push notification service for important events, used to send alerts to other systems or operators via email/SMS. CloudWatch Monitor resources and applications, collect logs, and set up alarms to ensure system stability and detect problems early. Analyze \u0026amp; Visualization Quicksight Business intelligence (BI) services to visualize data from RDS and Kinesis (after it has been processed), helping management monitor system status in real-time and make decisions. Automation develop CodePipeline Continuous Integration and Delivery (CI/CD) services to automate the entire process of building, testing, and deploying application source code. CodeBuild The service builds source code, runs automated tests, and generates deployment-ready packages. CodeDeploy The service automates the deployment of source code to EC2 instances or other environments, ensuring smooth, uninterrupted updates. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "Part 1: Networking \u0026amp; Security 1.1 Create VPC Steps:\nGo to VPC Console → Your VPCs → Create VPC Select VPC and more Name: metropolitano Create public/private subnets across at least 2 Availability Zones VPC Endpoints: None (baseline) 1.2 Enable Auto-assign Public IPv4 for Public Subnet Steps:\nVPC → Subnets → Select public subnet → Edit subnet settings Enable Auto-assign Public IPv4 → Save 1.3 Security Groups 1.3.1 Public Web/EC2 Security Group Name: public-web-sg\nInbound rules:\nHTTP/HTTPS from Internet SSH from admin IP only 1.3.2 Private Database Security Group Name: private-db-sg Inbound rules: MS SQL Server (1433) only from public-web-sg Part 2: Database Setup (RDS SQL Server) 2.1 Create DB Subnet Group RDS → Subnet Groups → Create Name: private-db-metropolitano VPC: metropolitano-vpc Subnets: private subnets across 2 AZs 2.2 Create RDS SQL Server Instance Steps:\nRDS → Databases → Create database\nEngine: Microsoft SQL Server Template: Dev/Test Credentials: Self-managed Public access: No Security group: private-db-sg Part 3: Compute – EC2 Application Server Steps:\nGo to EC2 → Instances → Launch instance 2. Name: metropolitano-version-1\n3. AMI: Amazon Linux\n4. Instance type: t3.medium\n5. Key pair: myKey.pm\n6. Network:\nVPC: metropolitano-vpc Subnet: Public subnet Security Group: public-web-sg 7. Wait until instance status = 3/3 checks passed Part 4: Storage – S3 Bucket 4.1 Create S3 Bucket Go to S3 → Buckets → Create bucket\nName: metropolitano-2025 Object Ownership: ACLs disabled (recommended) Block Public Access: Disable Block all public access Click Create bucket Upload the dist folder from frontend Part 5: CloudFront Steps:\nCloudFront Console → Distributions → Create distribution Plan: Free tier Name: Metropolitano Origin type: Amazon S3 Origin: metropolitano-2025 Origin path: /dist Cache settings: Viewer protocol policy: Redirect HTTP to HTTPS Allowed HTTP methods: GET, HEAD, OPTIONS, PUT, POST, PATCH, DELETE Wait 3–5 minutes for deployment.\n![Create cloudfront](/images/5-Workshop/5.4-S3-onprem/image_19.png)\r![Websites](/images/5-Workshop/5.4-S3-onprem/image_20.png)\rPart 6: Kinesis – Event Stream Steps:\nConsole → Kinesis → Create Data Stream 2. Name: metropolitano-stream\n3. Shards: Select based on expected data volume\n4. Producers (EC2) will send data; Consumers will process data Part 7: EventBridge – Event Automation Steps:\nConsole → EventBridge → Create Rule Name: metropolitano-event-rule Event source: AWS services (CloudWatch Alarm, S3 Object Created) Target: SQS, SNS Part 8: SQS – Message Queue Steps:\nConsole → SQS → Create Queue Name: metropolitano-queue Queue type: Standard or FIFO Part 9: SNS – Notification Service Steps:\nConsole → SNS → Create Topic Name: metropolitano-alerts Add subscriptions: Email, SMS Part 10: CloudWatch – Monitoring \u0026amp; Alerts Steps:\nConsole → CloudWatch → Create Alarm Metrics: EC2 CPU utilization RDS instance status Kinesis throughput Actions: Send notification via SNS when threshold is exceeded Part 11: Analytics \u0026amp; Visualization Steps:\nConsole → QuickSight → Sign up / Create account Dataset sources: RDS, S3, or Athena Create Analysis → Build dashboards \u0026amp; visual reports "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Generative AI-powered game design: Accelerating early development with Stability AI models on Amazon Bedrock Introduction In the competitive world of game development, staying ahead of technological advancements is crucial. Generative AI has emerged as a game changer, offering unprecedented opportunities for game designers to push boundaries and create immersive virtual worlds. At the forefront of this revolution is Stability AI’s cutting-edge text-to-image AI model, Stable Diffusion 3.5 Large (SD3.5 Large), which is transforming the way we approach game environment creation.\nSD3.5 Large, available in Amazon Bedrock, is Stability AI’s most advanced text-to-image model to date. With 8.1 billion parameters, this model excels at generating high-quality, 1-megapixel images from text descriptions with exceptional prompt adherence, making it ideal for creating detailed game environments at speed.\nIts improved architecture, based on the Multimodal Diffusion Transformer (MMDiT), combines multiple pre-trained text encoders for enhanced text understanding and uses QK-normalization to improve training stability.\nThe model demonstrates improved performance in image quality, typography, and complex prompt understanding. It excels at creating diverse, high-quality images across multiple styles, making it valuable for industries such as media, gaming, advertising, and education.\nIn this post, we explore how you can use SD3.5 Large to address practical gaming needs such as early concept art and character design.\nKey improvements in SD3.5 Large compared to SD3 Large SD3.5 Large offers the following improvements:\nEnhanced photorealism – Delivers detailed 3D imagery with unprecedented realism Superior scene complexity – Handles multiple subjects in intricate scenes with remarkable accuracy Improved anatomical rendering – Generates more precise and natural human representations Diverse representation – Creates images with inclusive representation of skin tones and features without extensive prompting Real-world use cases for game environment creation Image generation is poised to revolutionize a few key areas within the gaming industry. Firstly, it will significantly enhance the ideation and design process, allowing teams to rapidly create new scenes and objects, thereby accelerating the design cycle.\nSecondly, it will enable in-game content generation, empowering users to create new objects, modify avatar skins, or generate new textures. Although current adoption is more prevalent in the design phase, the continued advancement of generative AI is expected to lead to increased user-generated AI content (such as player avatars).\nThis shift towards AI-assisted content creation in gaming promises to open up new realms of possibilities for both developers and players alike.\nSample prompts for early game worlds A vibrant fantasy landscape featuring rolling hills, a sparkling river, and a majestic castle in the distance under a bright blue sky.\nA dense tropical rainforest teeming with exotic plants and wildlife, sunlight filtering through the thick canopy, with a hidden waterfall cascading into a crystal-clear pool.\nA futuristic city skyline at dusk, featuring sleek skyscrapers with neon lights and flying vehicles soaring between them, reflecting on the glassy surface of a river.\nSample prompts for game assets and props An intricately designed realistic game weapon prop of a fiery blue and green blade set against a blurred background of a gargantuan temple. The blade merges geometrical design with an alien cultural aesthetic.\nClose-up, side-angle view of an intricately designed realistic game weapon prop with a fiery blue and green blade against a blurred temple background.\nTop-down view of an intricately designed realistic game weapon prop with a fiery blue and green blade set against a blurred background of a gargantuan temple.\nSolution overview To demonstrate the power of SD3.5 Large in game environment creation, this post walks through a hypothetical workflow using a Jupyter notebook hosted in a GitHub repository.\nThe demo is designed to run in the us-west-2 AWS Region.\nPrerequisites This notebook is designed to run on AWS, using Amazon Bedrock to access both Anthropic’s Claude 3 Sonnet and Stability AI models.\nEnsure you have the following:\nAn AWS account An Amazon SageMaker domain Access to Stability AI’s SD3.5 Large text-to-image model through the Amazon Bedrock console Define the game world Start by outlining the core concepts of your game world, including its theme, atmosphere, and key locations.\nExample:\n“Mystic Realms is set in a vibrant fantasy world where players embark on quests to uncover ancient secrets and battle mystical creatures. The game features diverse environments, including enchanted forests, mystical mountains, and forgotten ruins. The atmosphere is whimsical and magical, with bright colors and fantastical elements that evoke a sense of wonder.”\nCraft detailed prompts for worlds and objects Use natural language to describe the environments and objects you want to create.\nYou can generate initial concept images using Amazon Bedrock:\nOpen the Amazon Bedrock console Under Foundation models, choose Model catalog Select Stability AI → Stable Diffusion 3.5 Large Choose Open in playground Enter your prompt and select Run A high-fidelity image will be generated in seconds.\nIterate and refine After generating an initial concept, create variations to explore different possibilities.\nAnalyze outputs and refine prompts by adjusting lighting, color palette, or environmental features. Use finalized images as visual references for 3D artists.\nClean up To avoid charges, stop active SageMaker notebook instances once the demo is complete.\nRefer to Clean up Amazon SageMaker notebook instance resources for details.\nConclusion Stability AI’s latest models represent a major advancement in generative AI, empowering game developers, designers, and content creators to enhance creative workflows and explore new dimensions of visual storytelling.\nBy adopting these models responsibly—considering bias, intellectual property, and misuse risks—gaming professionals can push the boundaries of what is possible in game design and content creation.\nTo get started, explore Stability AI models available in Amazon Bedrock.\nAbout the Authors Isha Dua is a Senior Solutions Architect based in the San Francisco Bay Area. She helps AWS Enterprise customers design cloud-native architectures that are resilient and scalable, with a strong interest in machine learning and environmental sustainability.\nParth Patel is a Senior Solutions Architect at AWS in the San Francisco Bay Area. He works with customers to accelerate cloud adoption and focuses on machine learning, environmental sustainability, and application modernization.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " Harness Amazon Q Business power with Microsoft SharePoint for enterprise search Introduction In today’s business landscape, organizations are looking for ways to extract insights from their growing data assets. Organizations rely heavily on their file server infrastructure to store, manage, and share mission-critical data. The volume and complexity of data creates challenges when working to maximize its value. Companies need a new strategy to overcome these obstacles.\nAmazon Q Business leverages Generative AI (GenAI) to address these data challenges. This service helps organizations use generative AI to improve decision-making and achieve business goals. Amazon Q Business integrates with existing data sources to reveal valuable insights.\nUsing GenAI, Amazon Q Business assists you in quickly analyzing your data, identifying patterns, and generating personalized recommendations tailored to your unique business needs. Whether you’re looking to improve customer service, optimize operational efficiency, or uncover new revenue opportunities, Amazon Q Business and GenAI will aid in driving innovation and growth.\nIn this blog post, we’ll demonstrate how Amazon Q Business integrates with Microsoft SharePoint Server to unlock the full potential of your files. You will learn how to query Microsoft SharePoint server data using natural language, find relevant information, extract key points, and derive valuable insights.\nSolution Overview Ensuring the confidentiality, integrity, and availability of data is of utmost importance. The Amazon Q Business connector for SharePoint employs a robust security framework that honors existing user identities, roles, and permissions. Identity crawling and access control lists (ACLs) are implemented on the connector using secure credentials managed by AWS Secrets Manager. The solution enforces the principle of least privilege, allowing users access only to the data for which they have explicit permissions.\nIn environments using Microsoft products, user and group objects are often stored in Microsoft Active Directory. Q Business synchronizes this information into AWS IAM Identity Center, enabling fine-grained access control and filtered query responses according to user permissions.\nThe SharePoint Server data connector ingests document content and NTFS/SharePoint permissions to provide a complete understanding of data access. When a user submits a query, the solution generates filtered responses that enforce permissions, ensuring sensitive data is accessed only by authorized individuals.\nNote: This solution is for SharePoint Server and not applicable to SharePoint Online.\nPrerequisites A working SharePoint environment deployed on Amazon EC2. How to deploy SharePoint server on Amazon EC2 AWS IAM Identity Center configured, along with an IAM role and user with permissions to manage Q Business resources. Onboard Amazon Q Business. AWS Directory Service for Microsoft Active Directory domain-joined SharePoint Server. One Amazon EC2 Windows instance with Remote Server Administrative Tools (RSAT) for managing AWS Managed AD users. AWS Managed AD as source of truth for AWS Identity Center. Secrets in AWS Secrets Manager with access to SharePoint Server credentials. Self-managed AD is also a viable alternative integration option.\nWalkthrough Configure Amazon Q Business Application Sign in to the Amazon Q Business console. Select Create application. Enter application name and select Web experience. Choose IAM Identity Center for Access management. Configure Application service access: Create and use a new service-linked role (SLR). Leave encryption default (AWS KMS key). Choose Create and open web experience to deploy application. Connect to SharePoint Navigate to Data sources → Add index. Create new index: Choose Enterprise index and number of units (e.g., 50). Amazon Q Business charges by document capacity. Add SharePoint data source: Select SharePoint Server version. Enter site URL, domain, and SSL certificate S3 path. Configure Authorization (ACLs) and Authentication (NTLM/Kerberos). Create or choose AWS Secrets Manager secret for credentials. Create IAM role for data source. Set Sync scope, mode (Full Sync), and schedule (Daily). Click Sync now to crawl and index data. Access Application Users access the Amazon Q Business application via its Web URL.\nTest the Solution Scenario: HR reviews resumes to select a “Cyber Security Strategist.”\nHR Admin queries using NLP in Amazon Q AI assistant → receives full results. IT Admin (SP_Gary) queries → receives no results due to restricted permissions. Demonstrates principle of least privilege and ACL enforcement.\nCleanup Delete Q Business application, Data source, AWS Managed AD, EC2 management server, Secrets, etc., to avoid charges. Conclusion You learned how to configure the SharePoint connector for Amazon Q Business using least privilege principles. Employees can securely interact with organizational knowledge in SharePoint using natural language, improving productivity, decision-making, and knowledge sharing. Multiple data connectors can be integrated similarly for other use cases.\nAWS provides more services and features than any other cloud provider, enabling faster, easier, and cost-effective migration and modernization.\nAWS Gen AI Services Amazon Q – Generative AI Assistance Amazon Q Business Amazon Q Developer Supported data connector Amazon Q Business with SharePoint Online Amazon Bedrock Authors Mangesh Budkule – Senior Specialist Solution Architect at AWS, expert in GenAI and Microsoft workloads.\nJarod Oliver – Specialist Solutions Architect, focuses on containers and GenAI.\nSiavash Irani – Principal Solutions Architect at AWS, specializes in Microsoft workloads and EC2Rescue for Windows.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": "Ground truth generation and review best practices for evaluating generative AI question-answering with FMEval Generative AI question-answering applications are pushing the boundaries of enterprise productivity. These assistants can be powered by various backend architectures including Retrieval Augmented Generation (RAG), agentic workflows, fine-tuned large language models (LLMs), or a combination of these techniques. However, building and deploying trustworthy AI assistants requires a robust ground truth and evaluation framework.\nGround truth data in AI refers to data that is known to be factual, representing the expected use case outcome for the system being modeled. By providing an expected outcome to measure against, ground truth data unlocks the ability to deterministically evaluate system quality. Running deterministic evaluation of generative AI assistants against use case ground truth data enables the creation of custom benchmarks.\nThe Importance of Ground Truth These benchmarks are essential for tracking performance drift over time and for statistically comparing multiple assistants in accomplishing the same task. Additionally, they enable quantifying performance changes as a function of enhancements to the underlying assistant, all within a controlled setting. With deterministic evaluation processes such as the Factual Knowledge and QA Accuracy metrics of FMEval, ground truth generation and evaluation metric implementation are tightly coupled.\nIn this post, we discuss best practices for applying LLMs to generate ground truth for evaluating question-answering assistants with FMEval on an enterprise scale. FMEval is a comprehensive evaluation suite from Amazon SageMaker Clarify, and provides standardized implementations of metrics to assess quality and responsibility.\nGenerating Ground Truth for FMEval Question-Answering Evaluation One option to get started with ground truth generation is human curation of a small question-answer dataset. The human curated dataset should be small (based on bandwidth), high in signal, and ideally prepared by use case subject matter experts (SMEs).\nOutcomes of Initial Ground Truth Generation The exercise of generating this dataset forces a data alignment exercise early in the evaluation process. The outcomes for this exercise are three-fold:\nStakeholder alignment on the top N important questions Stakeholder awareness of the evaluation process A high-fidelity starter ground truth dataset for the first proof of concept evaluation Scaling with LLMs While an SME ground truth curation exercise is a strong start, at the scale of an enterprise knowledge base, pure SME generation of ground truth will become prohibitively time and resource intensive. To scale ground truth generation and curation, you can apply a risk-based approach in conjunction with a prompt-based strategy using LLMs.\nImportant: LLM-generated ground truth isn\u0026rsquo;t a substitute for use case SME involvement. SMEs will still be needed to identify which questions are fundamental to the business and align the ground truth with business value as part of a human-in-the-loop process.\nExample: Amazon 2023 Shareholder Letter To demonstrate, we provide a step-by-step walkthrough using Amazon\u0026rsquo;s 2023 letter to shareholders as source data. In keeping with ground truth curation best practices for FMEval question-answering, ground truth is curated as question-answer-fact triplets.\nSource Document Dear Shareholders: Last year at this time, I shared my enthusiasm and optimism for Amazon\u0026rsquo;s future. Today, I have even more. The reasons are many, but start with the progress we\u0026rsquo;ve made in our financial results and customer experiences, and extend to our continued innovation and the remarkable opportunities in front of us. In 2023, Amazon\u0026rsquo;s total revenue grew 12% year-over-year (\u0026ldquo;YoY\u0026rdquo;) from $514B to $575B. By segment, North America revenue increased 12% YoY from $316B to $353B, International revenue grew 11% YoY from $118B to $131B, and AWS revenue increased 13% YoY from $80B to $91B.\nGenerated Ground Truth Examples Question Ground Truth Answer Fact What was Amazon\u0026rsquo;s total revenue growth in 2023? Amazon\u0026rsquo;s total revenue grew 12% year-over-year from $514B to $575B in 2023. 12%$514B to $575B How much did North America revenue increase in 2023? North America revenue increased 12% year-over-year from $316B to $353B. 12%$316B to $353B What was the growth in International revenue for Amazon in 2023? International revenue grew 11% year-over-year from $118B to $131B. 11%$118B to $131B How much did AWS revenue increase in 2023? AWS revenue increased 13% year-over-year from $80B to $91B. 13%$80B to $91B What was Amazon\u0026rsquo;s operating income improvement in 2023? Operating income in 2023 improved 201% year-over-year from $12.2B to $36.9B. 201%$12.2B to $36.9B What was Amazon\u0026rsquo;s operating margin in 2023? Amazon\u0026rsquo;s operating margin in 2023 was 6.4%. 6.4% LLM Prompt Template for Ground Truth Generation To convert the source document excerpt into ground truth, we provide a base LLM prompt template. For our example, we work with Anthropic\u0026rsquo;s Claude LLM on Amazon Bedrock.\nYou are an expert in ground truth curation for generative AI application evaluation on AWS. Follow the instructions provided in the \u0026lt;instructions\u0026gt; XML tag for generating question answer fact triplets from a source document excerpt. \u0026lt;instructions\u0026gt; - Let\u0026#39;s work this out in a step-by-step way to be sure we have the right answer. - Review the source document excerpt provided in \u0026lt;document\u0026gt; XML tags below - For each meaningful domain fact in the \u0026lt;document\u0026gt;, extract an unambiguous question-answer-fact set in JSON format including a question and answer pair encapsulating the fact in the form of a short sentence, followed by a minimally expressed fact extracted from the answer. \u0026lt;domain_knowledge_focus\u0026gt; - Focus ONLY on substantive domain knowledge contained within the document content - Ignore all metadata and structural elements including but not limited to: - Document dates, versions, page numbers - Section numbers or titles - Table structure or row/column positions - List positions or ordering - Questions must reference specific domain entities rather than generic document elements \u0026lt;/domain_knowledge_focus\u0026gt; \u0026lt;best_practices\u0026gt; - Questions, answers, and facts should not refer to the subject entity as \u0026#34;it\u0026#34; or \u0026#34;they\u0026#34; - Facts should be represented in 3 or fewer words describing an entity - If there are units in the fact, provide multiple versions using \u0026lt;OR\u0026gt; as delimiter \u0026lt;unit_variations\u0026gt; - Dollar Unit Equivalencies: `1,234 million\u0026lt;OR\u0026gt;1.234 billion` - Date Format Equivalencies: `2024-01-01\u0026lt;OR\u0026gt;January 1st 2024` - Number Equivalencies: `1\u0026lt;OR\u0026gt;one` \u0026lt;/unit_variations\u0026gt; \u0026lt;/best_practices\u0026gt; - Start your response immediately with the question-answer-fact set JSON \u0026lt;/instructions\u0026gt; \u0026lt;document\u0026gt; {context_document} \u0026lt;/document\u0026gt; Now, extract the question answer pairs and fact from the document excerpt. Output Format The generation output is provided as fact-wise JSONLines records:\n{ \u0026#34;question\u0026#34;: \u0026#34;[Question]\u0026#34;, \u0026#34;ground_truth_answer\u0026#34;: \u0026#34;[Ground Truth Answer]\u0026#34;, \u0026#34;fact\u0026#34;: \u0026#34;[Fact]\u0026#34; } Scaling Ground Truth Generation with a Pipeline To automate ground truth generation, we provide a serverless batch pipeline architecture. At a high level, the AWS Step Functions pipeline accepts source data in Amazon S3, and orchestrates AWS Lambda functions for ingestion, chunking, and prompting on Amazon Bedrock.\nPipeline Input Parameters There are three user inputs to the step function:\n{ \u0026#34;dataset_name\u0026#34;: \u0026#34;YOUR_DATASET_NAME\u0026#34;, \u0026#34;input_prefix\u0026#34;: \u0026#34;YOUR_INPUT_PREFIX\u0026#34;, \u0026#34;review_percentage\u0026#34;: \u0026#34;REVIEW_PERCENTAGE\u0026#34; } Additional configurations are set by Lambda environment variables, such as:\nS3 source bucket Amazon Bedrock Model ID to invoke on generation Pipeline Event Payload Structure After validation, the system assembles the global event payload:\n{ \u0026#34;system_input\u0026#34;: { \u0026#34;run_id\u0026#34;: \u0026#34;\u0026lt;AWS Step Function execution ID\u0026gt;\u0026#34;, \u0026#34;input_bucket\u0026#34;: \u0026#34;\u0026lt;Input data Amazon S3 bucket\u0026gt;\u0026#34;, \u0026#34;output_bucket\u0026#34;: \u0026#34;\u0026lt;Output data Amazon S3 bucket\u0026gt;\u0026#34;, \u0026#34;output_document_chunks_prefix\u0026#34;: \u0026#34;\u0026lt;S3 Prefix to store chunks\u0026gt;\u0026#34;, \u0026#34;chunk_size\u0026#34;: \u0026#34;\u0026lt;Document chunk size\u0026gt;\u0026#34;, \u0026#34;chunk_overlap\u0026#34;: \u0026#34;\u0026lt;Number of tokens overlap\u0026gt;\u0026#34; }, \u0026#34;user_input\u0026#34;: { \u0026#34;dataset_name\u0026#34;: \u0026#34;\u0026lt;Dataset name\u0026gt;\u0026#34;, \u0026#34;input_prefix\u0026#34;: \u0026#34;\u0026lt;S3 prefix for input data\u0026gt;\u0026#34;, \u0026#34;review_percentage\u0026#34;: \u0026#34;\u0026lt;Percent for human review\u0026gt;\u0026#34; } } Pipeline Stages First Distributed Map: Iterates over files in the input bucket to start document ingestion and chunking with horizontal scaling Second Distributed Map: Each chunk is fed to the ground truth generation prompt on Amazon Bedrock Aggregation Step: SageMaker Processing job concatenates JSONLines records and samples for human review Judging Ground Truth for FMEval Question-Answering Evaluation Measuring ground truth quality is an essential component of the evaluation lifecycle. There are two key components:\n1. Human-in-the-Loop (HITL) The level of ground truth human review required is determined by the risk of having incorrect ground truth. The HITL process consists of four steps:\nStep 1: Classify Risk Perform a risk analysis to establish the severity and likelihood of negative events. Assign the ground truth dataset a risk level: Low, Medium, High, or Critical.\nRisk Assessment Matrix:\nLikelihood Very Low Low Moderate Major Extreme Frequent Low Medium High Critical Critical Likely Very low Low Medium High Critical Possible Very low Low Medium Medium High Unlikely Very low Very low Low Low Medium Rare Very low Very low Very low Very low Low Step 2: Human Review Based on the assigned risk level, use-case expert reviewers examine a proportional amount of the ground truth. Organizations can set acceptability thresholds for percentage of HITL intervention based on their tolerance for risk.\nStep 3: Identify Findings Reviewers identify:\nHallucinations relative to source data Information veracity challenges according to their expertise Other criteria set by the organization Step 4: Action Results Reviewers take business actions such as:\nUpdating and deleting records Re-writing applicable source documents Bringing in LLMOps SMEs to apply dataset curation best practices Example: Hallucination Detection Source Data Hallucinated Output Corrected Output Amazon\u0026rsquo;s total revenue grew 12% YoY from $514B to $575B \u0026ldquo;grew 15% year-over-year\u0026rdquo; \u0026ldquo;grew 12% year-over-year\u0026rdquo; Example: SME Veracity Review Source Data SME Review Remediation \u0026ldquo;Effective June 1st, 2023, AnyCompany is pleased to announce Casual Friday\u0026hellip;\u0026rdquo; \u0026ldquo;As an HR Specialist, this looks incorrect. We did not implement the Casual Friday policy—the source data must be out of date.\u0026rdquo; Delete incorrect ground truth; Update source document 2. LLM-as-a-Judge When scaling HITL, LLM reviewers can perform hallucination detection and remediation. This idea is known as self-reflective RAG, and can be used to decrease—but not eliminate—the level of human effort in the process.\nAmazon Bedrock now offers:\nThe ability to use LLM reviewers Automated reasoning checks with Amazon Bedrock Guardrails for mathematically sound self-validation Golden Rule: Make sure the organization\u0026rsquo;s review process aligns with the accepted risk level for the ground truth dataset.\nConclusion In this post, we provided guidance on generating and reviewing ground truth for evaluating question-answering applications using FMEval. We explored best practices for applying LLMs to scale ground truth generation while maintaining quality and accuracy.\nKey Takeaways Use SME-curated datasets as a starting point for high-quality ground truth Apply LLM-based generation to scale across enterprise knowledge bases Implement serverless batch pipelines for automation Establish risk-based HITL processes for quality assurance Leverage LLM-as-a-judge patterns for scalable review By following these guidelines, organizations can follow responsible AI best practices for creating high-quality ground truth datasets for deterministic evaluation of question-answering assistants. Use case-specific evaluations supported by well-curated ground truth play a crucial role in developing and deploying AI solutions that meet the highest standards of quality and responsibility.\nWhether you\u0026rsquo;re developing an internal tool, a customer-facing virtual assistant, or exploring the potential of generative AI for your organization, we encourage you to adopt these best practices. Start implementing robust ground truth generation and review processes for your generative AI question-answering evaluations today with FMEval.\nAbout the Authors Samantha Stuart is a Data Scientist with AWS Professional Services, and has delivered for customers across generative AI, MLOps, and ETL engagements. Samantha has a research master\u0026rsquo;s degree in engineering from the University of Toronto, where she authored several publications on data-centric AI for drug delivery system design. Outside of work, she is most likely spotted playing music, spending time with friends and family, at the yoga studio, or exploring Toronto.\nPhilippe Duplessis-Guindon is a cloud consultant at AWS, where he has worked on a wide range of generative AI projects. He has touched on most aspects of these projects, from infrastructure and DevOps to software development and AI/ML. After earning his bachelor\u0026rsquo;s degree in software engineering and a master\u0026rsquo;s in computer vision and machine learning from Polytechnique Montreal, Philippe joined AWS to put his expertise to work for customers. When he\u0026rsquo;s not at work, you\u0026rsquo;re likely to find Philippe outdoors—either rock climbing or going for a run.\nRahul Jani is a Data Architect with AWS Professional Service. He collaborates closely with enterprise customers building modern data platforms, generative AI applications, and MLOps. He is specialized in the design and implementation of big data and analytical applications on the AWS platform. Beyond work, he values quality time with family and embraces opportunities for travel.\nIvan Cui is a Data Science Lead with AWS Professional Services, where he helps customers build and deploy solutions using ML and generative AI on AWS. He has worked with customers across diverse industries, including software, finance, pharmaceutical, healthcare, IoT, and entertainment and media. In his free time, he enjoys reading, spending time with his family, and traveling.\nReferences Evaluate large language models for quality and responsibility of LLMs Generative AI Security Scoping Matrix Amazon Bedrock documentation on LLM prompt design Learn how to assess the risk of AI systems New RAG evaluation and LLM-as-a-judge capabilities in Amazon Bedrock "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Event Reflection: “Gen AI and Data Track – AWS Summit” Event Objectives The event focused on sharing strategies and practical approaches for adopting Generative AI (GenAI) on the AWS platform. The main objectives included:\nPresenting the roadmap and strategy for applying GenAI in enterprises Introducing how to build a unified data platform for analytics and AI Explaining the AI-driven software development lifecycle (AI-DLC) Sharing comprehensive security practices for GenAI applications Exploring the role of AI Agents in improving organizational productivity Speakers Jun Kai Loke – AI/ML Specialist Solutions Architect, AWS Kien Nguyen – Solutions Architect, AWS Tamelly Lim – Storage Specialist Solutions Architect, AWS Binh Tran – Senior Solutions Architect, AWS Taiki Dang – Solutions Architect, AWS Michael Armentano – Principal WW GTM Specialist, AWS Key Topics Building a Unified Data Platform on AWS for AI and Analytics The speakers explained how to design a scalable and unified data platform to support analytics and AI workloads. Key components included:\nCollecting data from multiple sources Cost-optimized data storage Data processing using services such as AWS Glue, Amazon EMR, and Amazon Redshift Data governance, access control, and security The goal of this approach is to help organizations manage data effectively and build a strong foundation for AI projects.\nGenAI Vision and Adoption Roadmap on AWS This session introduced AWS’s vision and current trends in Generative AI adoption. Key services included:\nAmazon Bedrock – enabling GenAI applications without managing infrastructure Amazon Q – an AI assistant designed for enterprises and developers The recommended strategy is to start with small pilot projects, then scale gradually while aligning with business objectives.\nAI-Driven Development Lifecycle (AI-DLC) AI-DLC represents a new approach to software development where AI plays a central role throughout the entire lifecycle. Instead of acting only as a support tool, AI becomes a collaborative partner in development.\nKey benefits include:\nFaster development cycles Improved software quality Increased creativity and innovation AI can automate tasks such as documentation writing, test case generation, and code improvement suggestions.\nSecuring GenAI Applications on AWS Security for GenAI applications must be implemented across multiple layers:\nInfrastructure layer: encryption and access control Model layer: managing model access and controlling inputs and outputs Application layer: protecting user data and detecting abnormal behavior Techniques discussed included zero-trust architecture, fine-grained access control, continuous monitoring, and auditing.\nAI Agents – Enhancing Productivity AI Agents were introduced as intelligent assistants capable of learning, adapting, and executing complex tasks autonomously. Compared to traditional automation, AI Agents are more flexible and context-aware.\nExample use cases include:\nCustomer support Logistics optimization Real-time data analysis Key Learnings Data Strategy A unified data platform is essential for effective AI adoption Data governance is as important as data storage and processing GenAI Roadmap There is no one-size-fits-all roadmap for GenAI adoption Amazon Bedrock and Amazon Q simplify the deployment of GenAI solutions AI-DLC AI is not just a tool but a development partner AI helps accelerate development and improve software quality Security GenAI security must be addressed at multiple layers Ethical AI practices are mandatory in all deployments AI Agents AI Agents are expected to become “digital workers” in the near future Organizations should prepare early to integrate them effectively Practical Applications Based on the event content, the following applications can be considered:\nBuilding AWS-based data platforms for AI and analytics projects Experimenting with Amazon Bedrock to develop internal GenAI applications Using Amazon Q Developer to support the software development lifecycle Applying AI-DLC to improve test case creation and code review processes Exploring AI Agents integration into business workflows to enhance productivity Event Experience Participating in the Gen AI and Data track at AWS Summit provided a comprehensive overview of how Generative AI is shaping modern enterprises. Learning directly from AWS experts helped clarify both strategic vision and practical implementation. The sessions on AI Agents and AI-DLC were especially insightful and encouraged further exploration of AI-driven development and automation.\nKey Takeaways GenAI is a foundation for transforming business processes, not just a standalone technology A secure and unified data platform is critical for successful GenAI adoption AI Agents are emerging as an important digital workforce Security and ethical considerations must always be prioritized "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: To connect and get acquainted with members of the First Cloud Journey team. To understand basic concepts of Amazon Web Services (AWS). To become familiar with the AWS Management Console and AWS Command Line Interface (CLI). To practice basic operations related to EC2 and storage services. Tasks to Be Carried Out During the Week: Day Task Start Date Completion Date Reference Material 2 - Introduced myself and communicated with FCJ members - Read and understood the rules and regulations of the internship program 08/09/2025 08/09/2025 — 3 - Studied an overview of AWS - Learned major AWS service categories: + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Created an AWS Free Tier account - Became familiar with AWS Console and AWS CLI - Practice: + Installed and configured AWS CLI + Set up Access Key and default Region 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Studied basic knowledge of Amazon EC2: + Instance types + Amazon Machine Image (AMI) + Elastic Block Store (EBS) - Learned different SSH connection methods to EC2 - Learned the concept of Elastic IP 11/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launched an EC2 instance + Connected to EC2 via SSH + Attached and verified an EBS volume 12/09/2025 12/09/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Gained a basic understanding of cloud computing concepts and the role of AWS.\nIdentified the core AWS service groups, including:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console, including how to locate and access basic services via the web interface.\nInstalled and configured AWS CLI on the local machine, including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nChecking account and configuration information Retrieving the list of available Regions Viewing EC2-related information Creating and managing key pairs Checking the status of running services Developed the ability to manage AWS resources in parallel using both the AWS Console and AWS CLI.\nGained initial hands-on experience with EC2 and EBS services.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic services\nWeek 2: In-depth study of EC2 and server management on AWS\nWeek 3: Working with Storage: Amazon S3 and EBS\nWeek 4: Basic networking with Amazon VPC\nWeek 5: Security and Access Management with IAM\nWeek 6: Deploying a basic application on AWS\nWeek 7: AWS Monitoring and Logging with CloudWatch\nWeek 8: Introduction to CI/CD and automation on AWS\nWeek 9: Infrastructure as Code with CloudFormation\nWeek 10: Cost optimization and AWS resource management\nWeek 11: Review, system completion, and troubleshooting\nWeek 12: Final summary, evaluation, and lessons learned\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "3.1 AWS account, Region \u0026amp; IAM Region + Choose region to deploy(ap-southeast-1) IAM baseline + Infra/Admin group: Administrator, Prefer to separate by environment (dev/stg/prod). + Stakeholders: ReadOnlyAccess + CI/CD (service roles): CodePipeline, CodeBuild, CodeDeploy + EC2 instance profile rule(run time): Read secrets from Secrets Manager follow prefix(example: metro//*),push log/metrics into CloudWatch, S3/Kinesis/SNS for function Security Notes + Do not give AdministratorAccess for EC2 Role + Using least privilege: policy for ARN resource + prefix + Turn the MFA for importmant IAM Users/roles + Activate the baseline guardrails {\r\u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;,\r\u0026#34;Statement\u0026#34;: [\r{\r\u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;,\r\u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;,\r\u0026#34;Action\u0026#34;: [\r\u0026#34;cloudformation:*\u0026#34;,\r\u0026#34;cloudwatch:*\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:AllocateAddress\u0026#34;,\r\u0026#34;ec2:AssociateAddress\u0026#34;,\r\u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;,\r\u0026#34;ec2:AssociateRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;,\r\u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;,\r\u0026#34;ec2:AttachInternetGateway\u0026#34;,\r\u0026#34;ec2:AttachNetworkInterface\u0026#34;,\r\u0026#34;ec2:AttachVolume\u0026#34;,\r\u0026#34;ec2:AttachVpnGateway\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;,\r\u0026#34;ec2:CreateClientVpnRoute\u0026#34;,\r\u0026#34;ec2:CreateCustomerGateway\u0026#34;,\r\u0026#34;ec2:CreateDhcpOptions\u0026#34;,\r\u0026#34;ec2:CreateFlowLogs\u0026#34;,\r\u0026#34;ec2:CreateInternetGateway\u0026#34;,\r\u0026#34;ec2:CreateLaunchTemplate\u0026#34;,\r\u0026#34;ec2:CreateNetworkAcl\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterface\u0026#34;,\r\u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:CreateRoute\u0026#34;,\r\u0026#34;ec2:CreateRouteTable\u0026#34;,\r\u0026#34;ec2:CreateSecurityGroup\u0026#34;,\r\u0026#34;ec2:CreateSubnet\u0026#34;,\r\u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:CreateTags\u0026#34;,\r\u0026#34;ec2:CreateTransitGateway\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:CreateVpc\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpoint\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;,\r\u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;,\r\u0026#34;ec2:CreateVpnConnection\u0026#34;,\r\u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:CreateVpnGateway\u0026#34;,\r\u0026#34;ec2:DeleteCustomerGateway\u0026#34;,\r\u0026#34;ec2:DeleteFlowLogs\u0026#34;,\r\u0026#34;ec2:DeleteInternetGateway\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterface\u0026#34;,\r\u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;,\r\u0026#34;ec2:DeleteRoute\u0026#34;,\r\u0026#34;ec2:DeleteRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteSecurityGroup\u0026#34;,\r\u0026#34;ec2:DeleteSubnet\u0026#34;,\r\u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;,\r\u0026#34;ec2:DeleteTags\u0026#34;,\r\u0026#34;ec2:DeleteTransitGateway\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;,\r\u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:DeleteVpc\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpoints\u0026#34;,\r\u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnection\u0026#34;,\r\u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;,\r\u0026#34;ec2:Describe*\u0026#34;,\r\u0026#34;ec2:DetachInternetGateway\u0026#34;,\r\u0026#34;ec2:DisassociateAddress\u0026#34;,\r\u0026#34;ec2:DisassociateRouteTable\u0026#34;,\r\u0026#34;ec2:GetLaunchTemplateData\u0026#34;,\r\u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;,\r\u0026#34;ec2:ModifyInstanceAttribute\u0026#34;,\r\u0026#34;ec2:ModifySecurityGroupRules\u0026#34;,\r\u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;,\r\u0026#34;ec2:ModifyVpcAttribute\u0026#34;,\r\u0026#34;ec2:ModifyVpcEndpoint\u0026#34;,\r\u0026#34;ec2:ReleaseAddress\u0026#34;,\r\u0026#34;ec2:ReplaceRoute\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;,\r\u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;,\r\u0026#34;ec2:RunInstances\u0026#34;,\r\u0026#34;ec2:StartInstances\u0026#34;,\r\u0026#34;ec2:StopInstances\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;,\r\u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;,\r\u0026#34;iam:AddRoleToInstanceProfile\u0026#34;,\r\u0026#34;iam:AttachRolePolicy\u0026#34;,\r\u0026#34;iam:CreateInstanceProfile\u0026#34;,\r\u0026#34;iam:CreatePolicy\u0026#34;,\r\u0026#34;iam:CreateRole\u0026#34;,\r\u0026#34;iam:DeleteInstanceProfile\u0026#34;,\r\u0026#34;iam:DeletePolicy\u0026#34;,\r\u0026#34;iam:DeleteRole\u0026#34;,\r\u0026#34;iam:DeleteRolePolicy\u0026#34;,\r\u0026#34;iam:DetachRolePolicy\u0026#34;,\r\u0026#34;iam:GetInstanceProfile\u0026#34;,\r\u0026#34;iam:GetPolicy\u0026#34;,\r\u0026#34;iam:GetRole\u0026#34;,\r\u0026#34;iam:GetRolePolicy\u0026#34;,\r\u0026#34;iam:ListPolicyVersions\u0026#34;,\r\u0026#34;iam:ListRoles\u0026#34;,\r\u0026#34;iam:PassRole\u0026#34;,\r\u0026#34;iam:PutRolePolicy\u0026#34;,\r\u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;,\r\u0026#34;lambda:CreateFunction\u0026#34;,\r\u0026#34;lambda:DeleteFunction\u0026#34;,\r\u0026#34;lambda:DeleteLayerVersion\u0026#34;,\r\u0026#34;lambda:GetFunction\u0026#34;,\r\u0026#34;lambda:GetLayerVersion\u0026#34;,\r\u0026#34;lambda:InvokeFunction\u0026#34;,\r\u0026#34;lambda:PublishLayerVersion\u0026#34;,\r\u0026#34;logs:CreateLogGroup\u0026#34;,\r\u0026#34;logs:DeleteLogGroup\u0026#34;,\r\u0026#34;logs:DescribeLogGroups\u0026#34;,\r\u0026#34;logs:PutRetentionPolicy\u0026#34;,\r\u0026#34;route53:ChangeTagsForResource\u0026#34;,\r\u0026#34;route53:CreateHealthCheck\u0026#34;,\r\u0026#34;route53:CreateHostedZone\u0026#34;,\r\u0026#34;route53:CreateTrafficPolicy\u0026#34;,\r\u0026#34;route53:DeleteHostedZone\u0026#34;,\r\u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;,\r\u0026#34;route53:GetHostedZone\u0026#34;,\r\u0026#34;route53:ListHostedZones\u0026#34;,\r\u0026#34;route53domains:ListDomains\u0026#34;,\r\u0026#34;route53domains:ListOperations\u0026#34;,\r\u0026#34;route53domains:ListTagsForDomain\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:AssociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:CreateResolverRule\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:DeleteResolverRule\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;,\r\u0026#34;route53resolver:DisassociateResolverRule\u0026#34;,\r\u0026#34;route53resolver:GetResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:GetResolverRule\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;,\r\u0026#34;route53resolver:ListResolverEndpoints\u0026#34;,\r\u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;,\r\u0026#34;route53resolver:ListResolverRules\u0026#34;,\r\u0026#34;route53resolver:ListTagsForResource\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;,\r\u0026#34;route53resolver:UpdateResolverRule\u0026#34;,\r\u0026#34;s3:AbortMultipartUpload\u0026#34;,\r\u0026#34;s3:CreateBucket\u0026#34;,\r\u0026#34;s3:DeleteBucket\u0026#34;,\r\u0026#34;s3:DeleteObject\u0026#34;,\r\u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetBucketAcl\u0026#34;,\r\u0026#34;s3:GetBucketOwnershipControls\u0026#34;,\r\u0026#34;s3:GetBucketPolicy\u0026#34;,\r\u0026#34;s3:GetBucketPolicyStatus\u0026#34;,\r\u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:GetObject\u0026#34;,\r\u0026#34;s3:GetObjectVersion\u0026#34;,\r\u0026#34;s3:GetBucketVersioning\u0026#34;,\r\u0026#34;s3:ListAccessPoints\u0026#34;,\r\u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;,\r\u0026#34;s3:ListAllMyBuckets\u0026#34;,\r\u0026#34;s3:ListBucket\u0026#34;,\r\u0026#34;s3:ListBucketMultipartUploads\u0026#34;,\r\u0026#34;s3:ListBucketVersions\u0026#34;,\r\u0026#34;s3:ListJobs\u0026#34;,\r\u0026#34;s3:ListMultipartUploadParts\u0026#34;,\r\u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;,\r\u0026#34;s3:ListStorageLensConfigurations\u0026#34;,\r\u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutBucketAcl\u0026#34;,\r\u0026#34;s3:PutBucketPolicy\u0026#34;,\r\u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;,\r\u0026#34;s3:PutObject\u0026#34;,\r\u0026#34;secretsmanager:CreateSecret\u0026#34;,\r\u0026#34;secretsmanager:DeleteSecret\u0026#34;,\r\u0026#34;secretsmanager:DescribeSecret\u0026#34;,\r\u0026#34;secretsmanager:GetSecretValue\u0026#34;,\r\u0026#34;secretsmanager:ListSecrets\u0026#34;,\r\u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;,\r\u0026#34;secretsmanager:PutResourcePolicy\u0026#34;,\r\u0026#34;secretsmanager:TagResource\u0026#34;,\r\u0026#34;secretsmanager:UpdateSecret\u0026#34;,\r\u0026#34;sns:ListTopics\u0026#34;,\r\u0026#34;ssm:DescribeInstanceProperties\u0026#34;,\r\u0026#34;ssm:DescribeSessions\u0026#34;,\r\u0026#34;ssm:GetConnectionStatus\u0026#34;,\r\u0026#34;ssm:GetParameters\u0026#34;,\r\u0026#34;ssm:ListAssociations\u0026#34;,\r\u0026#34;ssm:ResumeSession\u0026#34;,\r\u0026#34;ssm:StartSession\u0026#34;,\r\u0026#34;ssm:TerminateSession\u0026#34;\r],\r\u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34;\r}\r]\r} "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Event Summary: “Building Agentic AI – Context Optimization with Amazon Bedrock” Event Objectives The event aimed to provide an end-to-end overview of how to design and implement Agentic AI systems using Amazon Bedrock, combining architectural concepts with hands-on practices. The key objectives included:\nPresenting a structured journey for Agentic AI, from high-level architecture to practical implementation Introducing core concepts for building autonomous AI agents with Amazon Bedrock Sharing real-world use cases of agentic workflows on AWS Exploring agentic orchestration and context optimization strategies (L300 session) Creating opportunities for hands-on learning through workshops and professional networking Speakers Nguyen Gia Hung – Head of Solutions Architect Kien Nguyen – Solutions Architect Viet Pham – Founder \u0026amp; CEO Thang Ton – Co-Founder \u0026amp; COO Henry Bui – Head of Engineering Kha Van – Community Leader Key Content Event Overview and Logistics Date: December 5, 2025 Location: 26th Floor, Bitexco Financial Tower, Ho Chi Minh City, Vietnam Theme: Building autonomous AI agents using Amazon Bedrock through practical use cases and hands-on experience AWS Bedrock Agent Core This session introduced the foundational building blocks required for designing agentic systems on Amazon Bedrock, including:\nSelecting appropriate foundation models for specific problem domains Integrating tools or functions that allow agents to take actions Applying guardrails and constraints to improve safety and reliability Using tracing and evaluation to prepare systems for production environments Use Case: Building Agentic Workflows on AWS Speakers demonstrated how agentic workflows can be applied to real-world scenarios:\nAgents can execute multi-step tasks rather than simple single-turn question–answer interactions Iterative loops of retrieval, reasoning, and action enable flexible agent behavior Human-in-the-loop checkpoints are essential for actions with high impact or risk CloudThinker – Agentic Orchestration \u0026amp; Context Optimization (L300) The L300 session focused on the importance of context quality in agentic systems:\nContext directly affects accuracy, latency, and operating costs Key approaches to context optimization include: Maintaining minimal yet sufficient context to reduce noise and hallucinations Separating short-term context from long-term memory or knowledge bases Improving retrieval quality through metadata-based filtering (scope, recency, relevance) Applying context policies such as allowlists, redaction, and access control CloudThinker Hack: Hands-on Workshop Participants were guided through building a complete agentic workflow:\nDefining the agent’s goal Designing task steps and orchestration logic Integrating tools and testing agent behavior Iterating to improve reliability and performance The workshop also emphasized debugging techniques:\nInspecting prompts and tool calls Verifying retrieval results to reduce unnecessary context Adding safety constraints and guardrails Networking and Expert Discussions The networking session enabled in-depth discussions on:\nReal-world deployment constraints such as security, governance, and cost control Observability practices including logging and tracing Quality evaluation strategies and integration into existing engineering workflows Key Takeaways Agentic AI Mindset Agentic AI is more than a chatbot; it requires orchestration, tool usage, memory, and guardrails Context is a critical lever that determines both output quality and operational cost Technical Approach Clear separation of responsibilities is essential: Planner / Orchestrator Tools and actions Memory and retrieval Guardrails and safety controls Context optimization reduces hallucinations and improves system controllability Responsible Deployment Human-in-the-loop remains important for high-risk or high-impact actions Success metrics such as accuracy, latency, and cost must be clearly defined and continuously evaluated Practical Applications Based on the event content, potential applications in real work environments include:\nPrototyping internal agents for: Incident triage Runbook-guided troubleshooting Log summarization and analysis Applying context optimization techniques: Tightening retrieval scope Reducing prompt and context bloat Using structured memory and validation checkpoints Strengthening governance: Implementing guardrails for tool execution Establishing approval workflows for critical actions Enabling logging and tracing for compliance and debugging Event Experience The event provided a practical, end-to-end perspective on building agentic AI systems, effectively connecting architectural design with real implementation challenges. The L300 session clearly demonstrated how context optimization improves system reliability and cost efficiency. The hands-on workshop reinforced best practices in designing, testing, and debugging agentic workflows. Networking discussions helped relate theoretical concepts to real-world operational constraints.\nLessons Learned Context quality is as important as model selection when building AI agents Agentic systems should be treated as production-grade systems with safety, auditability, and evaluation Human-in-the-loop is still necessary for autonomous actions with significant impact Rapid iteration through trace → diagnosis → refinement is key to improving agent behavior "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "AWS First Cloud AI Journey – Metropolitano Railways Project Team: FPT HCM University\nClient: Metropolitano Railway Systems\nDocument Date: 12/09/2025\nTABLE OF CONTENTS BACKGROUND AND MOTIVATION\n1.1 EXECUTIVE SUMMARY\n1.2 PROJECT SUCCESS CRITERIA\n1.3 ASSUMPTIONS SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM\n2.1 TECHNICAL ARCHITECTURE DIAGRAM\n2.2 TECHNICAL PLAN\n2.3 PROJECT PLAN\n2.4 SECURITY CONSIDERATIONS ACTIVITIES AND DELIVERABLES\n3.1 ACTIVITIES AND DELIVERABLES\n3.2 OUT OF SCOPE\n3.3 PATH TO PRODUCTION EXPECTED AWS COST BREAKDOWN BY SERVICES TEAM RESOURCES \u0026amp; COST ESTIMATES ACCEPTANCE 1. BACKGROUND AND MOTIVATION 1.1 EXECUTIVE SUMMARY Client background:\nRapid urbanization is increasing pressure on public transportation systems. Metropolitano Railway Systems is modernizing operations to improve reliability, passenger experience, and operational efficiency. Current on-premises systems face challenges in scalability, high availability, and real-time data processing.\nBusiness \u0026amp; Technical Objectives:\nEnsure high availability and 24/7 operations for mission-critical rail services. Enable elastic scalability to handle peak passenger loads and seasonal demand. Improve system reliability and disaster recovery capabilities. Support real-time data processing for ticketing and payment transaction monitoring, including payment status tracking and reconciliation. Strengthen security \u0026amp; compliance aligned with public-sector transportation standards. Reduce operational costs through cloud-native automation. Accelerate innovation cycles for digital services such as mobile ticketing and predictive maintenance. Use cases (POC scope):\nDigital ticketing \u0026amp; fare collection (web/app booking, QR/IC card integration). Train scheduling \u0026amp; dispatch management (contextual use case; not the primary focus of this POC). Real-time payment/transaction event analytics through Amazon Kinesis (e.g., payment status updates, settlement events, anomaly detection). BI dashboards for management decision-making using Amazon QuickSight. Incident response and alerting through centralized monitoring and logging. Summary of Professional Services (Project Team):\nDesign an AWS architecture that is secure, scalable, and fault-tolerant based on Amazon EC2 and managed services. Migrate selected workloads from on-premises infrastructure to AWS (POC scope). Implement real-time payment/ticketing event pipelines using Amazon Kinesis, with storage and analytics datasets delivered to Amazon S3. Deploy analytics using QuickSight. Set up CI/CD using CodePipeline, CodeBuild, and CodeDeploy. Provide knowledge transfer/training to ensure operational readiness for the Client’s technical team. 1.2 PROJECT SUCCESS CRITERIA Service Reliability \u0026amp; Availability\nThe deployed system must achieve ≥ 99.9% uptime across all mission-critical services in scope (ticketing, payment monitoring, analytics). Scalability \u0026amp; Performance\nThe platform must automatically scale to handle peak passenger traffic without performance degradation. End-to-end response time for booking and payment APIs must remain under 300 ms during peak load tests (target). Real-Time Data Processing\nReal-time payment and ticketing transaction events must be processed with a latency under 5 seconds using Amazon Kinesis. Data ingestion pipelines must support at least 10,000 events/second with auto-scaling (target). Analytics \u0026amp; Insights Delivery\nManagement dashboards (QuickSight) must provide accurate, refreshed datasets within ≤ 5 minutes of data arrival in S3. CI/CD \u0026amp; Operational Excellence\nAll application deployments must be executed via automated CI/CD pipelines with rollback capability. Cost Efficiency\nAWS cost optimization mechanisms (Auto Scaling, RI/Savings Plans, lifecycle policies) target a reduction of at least 20% compared to on-premises operations. Cost Explorer and Billing Alarms must be configured to prevent overruns. 1.3 ASSUMPTIONS Prerequisites \u0026amp; Dependencies\nThe Client provides timely access to required environments, systems, and personnel. The Client supplies necessary credentials, API documentation, and integration endpoints for on-premises/third-party systems. Existing operational data (ticketing, scheduling, payment transactions) is available and accessible for migration and integration. Third-party vendors (payment gateways, fare systems, transit card systems) offer stable APIs and complete documentation. Required AWS accounts/organizations/billing structures are set up prior to project start. The Client identifies Subject Matter Experts (SMEs) for each domain (operations, IT, ticketing, scheduling). Technical Constraints\nSome legacy systems may remain on-premises, requiring hybrid connectivity via VPN or Direct Connect. Existing applications may have non-cloud-optimized architectures, limiting modernization within the POC scope. Legacy data quality issues may affect migration accuracy and analytics output. Operational networks and payment/ticketing systems must support secure and reliable cloud connectivity. Real-time analytics performance depends on ingestion latency and stability from payment gateways and ticketing transaction sources. Business Constraints\nProject timelines may depend on internal Client approvals/procurement processes. The Client’s organizational readiness and staff availability may impact progress. Budget limitations may restrict scope. Certain regulatory/compliance requirements may limit data residency/retention options. Risks (high-level)\nIntegration risk (legacy undocumented behaviors). Data migration risk (inconsistent/incomplete legacy data). Operational risk (hybrid/on-prem hardware failures). Security risk (misconfigured third-party endpoints). Timeline \u0026amp; dependency risk (vendor approvals, API throughput). 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Figure 1. AWS Technical Architecture Diagram – Metropolitano Railways (POC)\nFigure 1 (POC Outcome \u0026amp; Architecture Summary):\nThe POC enables near real-time monitoring and reconciliation of ticketing/payment transactions. Payment status events are streamed into Amazon Kinesis and delivered to Amazon S3 (data lake) for analytics. Amazon QuickSight dashboards provide revenue insights (daily/monthly) and highlight unpaid/incomplete payments for follow-up. User traffic is routed via Route 53 → CloudFront → AWS WAF to the application on Amazon EC2, with transactional data stored in Amazon RDS (private subnet). Amazon CloudWatch supports monitoring and alerting. Asynchronous workflows are decoupled using SNS/SQS (and EventBridge where needed). Source control uses GitLab; CI/CD is implemented using CodePipeline/CodeBuild/CodeDeploy.\nProduction note (recommendations for production readiness):\nEnable Multi-AZ and place workloads in private subnets behind an ALB. Restrict egress using NAT Gateway and/or VPC Endpoints. Enforce TLS (ACM), least-privilege IAM, and store secrets in Secrets Manager/SSM. Enable CloudTrail/Config and strengthen logging (WAF/CloudFront logs). Use blue/green or canary deployments with rollback. 2.2 TECHNICAL PLAN Key activities include:\nInfrastructure provisioning using IaC templates to deploy: Amazon EC2 clusters for application workloads Amazon RDS for database services Amazon S3 for static content and data lake storage Amazon Route 53 for DNS routing Amazon CloudFront and AWS WAF for secure global content delivery Amazon Kinesis for real-time data ingestion Amazon SQS/SNS for asynchronous messaging Amazon EventBridge for event-driven workflows Amazon CloudWatch for logging, monitoring, alarms, and dashboards Amazon QuickSight for analytics and BI reporting This POC does not include AWS Lambda; streaming consumers and asynchronous/background processing are implemented on EC2-based services (or containers) and integrated via SNS/SQS/EventBridge where applicable. Application build and release processes will be automated using CodePipeline, CodeBuild, and CodeDeploy, enabling blue/green or rolling deployments with automated rollback. Configuration items requiring approvals (production DNS changes, WAF rule modifications, RDS parameter adjustments, security group updates) will follow the Client’s Change Management Process (including CAB approvals and tracked deployment records). Critical paths (ticketing workflows, payment integrations, real-time ingestion, analytics refresh flows) will undergo unit, integration, load, and failover testing. Test scenarios, acceptance criteria, and validation procedures are provided in Appendix X. 2.3 PROJECT PLAN Stakeholder Participation:\nClient stakeholders (Operations, IT, Data, Security teams) are required for: Sprint Reviews (demo and feedback) Sprint Retrospectives (continuous improvement) UAT and sign-off sessions Technical design workshops Team Responsibilities (high-level):\nCloud Architect – AWS solution design, security architecture, scalability \u0026amp; HA patterns DevOps Engineer – CI/CD pipelines, IaC, automated deployments Application Engineer – application refactoring and integration Client IT Lead – access provisioning, governance alignment Client Operations Team – process validation, UAT testing Client Security Officer – compliance and security controls review Data Engineer – Kinesis pipelines, SQS/SNS integration, data modeling Analytics/BI Engineer – QuickSight dashboards and dataset automation Communication Cadence:\nDaily: standups with Project Team Weekly: project status updates to Client stakeholders Bi-weekly: sprint review + sprint planning Monthly: steering committee meeting Ad-hoc: incident response, change approvals, design deep-dives Knowledge Transfer:\nAWS architecture overview CI/CD pipeline management Monitoring and incident response using CloudWatch Data pipeline operations (Kinesis → S3 → QuickSight) Infrastructure lifecycle \u0026amp; IaC updates Security and IAM operations 2.4 SECURITY CONSIDERATIONS Access Security\nImplement least-privilege IAM, role-based access control, and best-practice IAM policies. Enable MFA for privileged accounts. Restrict CI/CD access using scoped IAM roles. Apply change approvals for Route 53 DNS, CloudFront updates, and WAF rule modifications. Infrastructure Security\nDeploy Amazon EC2 in private subnets within a Multi-AZ VPC architecture (production target). Use AWS WAF to protect CloudFront distributions and application endpoints against common exploits. Enforce traffic boundaries via Security Groups and NACLs. Run Amazon RDS in Multi-AZ with encrypted storage and automated backups. Use EventBridge for security-trigger automation (e.g., config rule violations), where applicable. Data Security\nEncrypt data at rest in S3, RDS, Kinesis, and SQS using AWS KMS CMKs. Protect data in transit using TLS 1.2+ across all services. Configure S3 lifecycle policies and object versioning for retention compliance. Data lake structure follows separation of raw, processed, curated layers. Detection \u0026amp; Monitoring\nEnable AWS CloudTrail and AWS Config for full API auditing and compliance tracking. CloudWatch collects logs, metrics, application traces, alarms, and dashboards. Deliver WAF logs and CloudFront access logs to S3 for security analytics. Incident Management\nDesign incident response playbooks for system failures, security breaches, data exposure, and pipeline errors. Trigger automated alerts using SNS/EventBridge to the Client’s operations team. Implement disaster recovery procedures for critical EC2 and RDS workloads. Ensure backup snapshots comply with Client-defined retention policies. Route operational alarms to the correct on-call teams for faster MTTR. 3. ACTIVITIES AND DELIVERABLES 3.1 ACTIVITIES AND DELIVERABLES Timeline basis: Internship duration 08/09/2025 – 22/11/2025 (~11 weeks)\nProject Phase Timeline Activities Deliverables / Milestones Total man-day Assessment Week 1–2 (08/09–21/09) Requirements workshops (business/technical/security); current-state analysis; identify integration points (payments, ticketing, analytics); environment readiness validation Assessment report; Architecture blueprint v1; Backlog \u0026amp; sprint plan TBD Setup base infrastructure Week 3–4 (22/09–05/10) VPC/subnets/routing/SGs; IAM baseline; S3 data lake foundation; CloudFront + WAF + Route53; RDS setup; CloudWatch Infrastructure provisioned; IaC templates delivered; Networking \u0026amp; security baseline TBD Setup component 1 Week 5–6 (06/10–19/10) EC2 Auto Scaling setup; deploy application backend; configure CI/CD (CodePipeline/CodeBuild/CodeDeploy); observability dashboards Application deployed; CI/CD pipelines operational; Monitoring dashboard TBD Setup component 2 Week 7–8 (20/10–02/11) Kinesis streaming pipelines; SQS/SNS messaging; ETL to S3 data lake; QuickSight dashboards; EventBridge workflows Data pipeline operational; Event-driven architecture; Revenue dashboards (daily/monthly); Unpaid/incomplete payments report (with alert thresholds) TBD Testing \u0026amp; Go-live (POC) Week 9–10 (03/11–16/11) Unit/integration/load testing; go-live readiness review; (POC) DNS cutover; monitoring \u0026amp; rozllback plans UAT sign-off; Go-live checklist; POC launch TBD Handover Week 11 (17/11–22/11) Final documentation; operations training; knowledge transfer sessions; transition to BAU Runbooks \u0026amp; SOPs; Admin training completion; Final acceptance TBD How to calculate man-day (guideline):\nMan-day = number of people × number of working days actually spent on the phase. Example: 5 people work 3 days on “Assessment” → 5 × 3 = 15 man-days. If you track by hours: 1 man-day ≈ 8 working hours (common convention). Example: total 120 hours → 120 / 8 = 15 man-days. 3.2 OUT OF SCOPE Custom application code development or feature enhancements not listed in the Scope of Work. On-premises infrastructure upgrades, network redesign, or hardware procurement. Performance tuning of third-party vendor systems. Mobile app development or UI/UX redesign outside the scope. Machine Learning model development beyond QuickSight \u0026amp; basic SageMaker patterns. Penetration testing or third-party security audits (unless contracted separately). Post go-live 24/7 operations unless contracted separately. Support for legacy networks or non-cloud-compatible components. Migration of data sources not included in the initial assessment. AWS Lambda–based serverless compute (event consumers/functions) is out of scope for this POC. 3.3 PATH TO PRODUCTION The Proof of Concept (POC) environment will demonstrate selected use cases defined in Section 2.2.\nThe POC environment will not contain all production-grade capabilities.\nKey gaps requiring enhancements before production deployment include:\nFull resilience design (multi-AZ, failover, scaling policies). Complete observability coverage (CloudWatch metrics, distributed tracing, WAF logs). Hardened security baselines (advanced IAM controls, WAF tuning, encryption policies). Comprehensive testing (integration, performance, DR simulation). CI/CD hardening and automated rollback. Production-approved DNS, WAF, and network change processes. Enhanced error handling and exception flows for edge cases. 4. EXPECTED AWS COST BREAKDOWN BY SERVICES Costing considerations (high-level):\nEC2 estimated using a mix of On-Demand + Reserved Instances/Savings Plans (production). Multi-AZ RDS with automated backups. CloudFront cost includes WAF usage (rule groups + request filtering). S3 includes storage tiers (Standard, Intelligent-Tiering) and lifecycle policies. Log storage and metrics in CloudWatch. Kinesis ingest \u0026amp; processing units. Data transfer cost between components. QuickSight Reader \u0026amp; Author licenses. Messaging (SQS, SNS, EventBridge) based on estimated throughput. Route 53 DNS queries + health checks. Assumptions (for accurate pricing):\nDaily ingestion volume: X GB/day via Kinesis. EC2 sizing based on projected user load for ticketing and payment workloads. S3 storage baseline calculated for 12 months retention. Moderate WAF rule usage and CloudFront regional edge charges. SQS/SNS/EventBridge traffic estimated from projected workflow processes. QuickSight usage includes 1 Author and X Readers. 5. TEAM Name Title Description Email / Contact Info Tào Bảo Thành Team Leader Manage the project, configure AWS services, code Backend, write documents, design architecture taobaothanh365@gmail.com Nguyễn Thị Nhã Uyên Member Design the UI and code Frontend uyenntnse183774@fpt.edu.vn Nguyễn Bảo Khánh Member Design the UI and code Frontend baokhanhtcv2005@gmail.com Trần Văn Quyết Member Code Backend quyettvse181574@fpt.edu.vn Nguyễn Văn Cường Member Code Backend cuongnvse183645@fpt.edu.vn "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: To gain deeper knowledge of Amazon EC2. To understand how to manage and operate virtual servers on AWS. To practice configuring and managing EC2 instances. Tasks to Be Carried Out During the Week: Day Task Start Date Completion Date Reference Material 2 - Reviewed EC2 fundamentals - Studied the EC2 instance lifecycle 15/09/2025 15/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learned in detail about EC2 instance types - Practiced selecting suitable instances based on workload 16/09/2025 16/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Studied Security Groups and Key Pairs - Learned basic EC2 access control 17/09/2025 17/09/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice: + Created and configured EC2 instances + Configured basic Security Group rules 18/09/2025 19/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Checked EC2 instance statuses - Performed Stop / Start / Terminate operations 19/09/2025 19/09/2025 https://docs.aws.amazon.com/ Week 2 Achievements: Gained deeper understanding of how Amazon EC2 works. Learned how to select EC2 instances based on specific use cases. Understood the role of Security Groups and Key Pairs in EC2 security. Successfully managed the EC2 instance lifecycle. Improved basic skills in operating virtual servers on AWS. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.3-s3-secrets-manager/",
	"title": "Create a secret manager",
	"tags": [],
	"description": "",
	"content": "Using AWS Secrets Manager In this section, you will create secrets manager to store a secret. The secrets manager will store the secrets key from your project like username and password from database, jwt secret key, and vnpay. This will useful to store on AWS and don\u0026rsquo;t need to worry about all secret will publish\nContent Create secret manager "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - Generative AI-powered game design: Accelerating early development with Stability AI models on Amazon Bedrock This blog explores how Stability AI\u0026rsquo;s Stable Diffusion 3.5 Large model on Amazon Bedrock revolutionizes game development workflows. You will learn how generative AI accelerates early-stage concept art creation, character design, and environment generation. The article demonstrates practical use cases including prompt engineering for fantasy landscapes, game assets, and props, while showcasing SD3.5 Large\u0026rsquo;s enhanced photorealism, superior scene complexity, and improved anatomical rendering capabilities. Step-by-step guidance covers setting up Amazon SageMaker, accessing models through Amazon Bedrock, and iterating on generated concepts to create high-fidelity visual references for 3D artists.\nBlog 2 - Harness Amazon Q Business power with Microsoft SharePoint for enterprise search This blog demonstrates how to integrate Amazon Q Business with Microsoft SharePoint Server to unlock enterprise knowledge using generative AI. You will learn how to configure secure data connectors that honor existing user permissions through ACLs and AWS IAM Identity Center synchronization. The article guides you through setting up SharePoint Server on Amazon EC2, configuring AWS Directory Service for Microsoft Active Directory, and implementing natural language queries with principle of least privilege enforcement. Real-world scenarios illustrate how different user roles access filtered results based on their SharePoint permissions, ensuring data confidentiality while improving productivity and decision-making across the organization.\nBlog 3 - Ground truth generation and review best practices for evaluating generative AI question-answering with FMEval This blog provides comprehensive best practices for generating and evaluating ground truth datasets to assess generative AI question-answering systems using Amazon SageMaker Clarify\u0026rsquo;s FMEval. You will learn how to scale ground truth creation through LLM-powered automation using Amazon Bedrock, implement serverless batch pipelines with AWS Step Functions and Lambda, and establish risk-based human-in-the-loop (HITL) review processes. The article covers question-answer-fact triplet curation, prompt engineering techniques for Claude on Bedrock, hallucination detection strategies, and LLM-as-a-judge patterns to ensure deterministic evaluation of RAG applications while maintaining high-quality benchmarks for enterprise knowledge assistants.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: To learn about AWS storage services. To understand the differences between Amazon S3 and Amazon EBS. To practice basic data and storage management. Tasks to Be Carried Out During the Week: Day Task Start Date Completion Date Reference Material 2 - Studied an overview of AWS storage services 22/09/2025 22/09/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learned Amazon S3 concepts and basic usage 23/09/2025 23/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Studied Amazon EBS - Compared S3 and EBS use cases 24/09/2025 24/09/2025 https://docs.aws.amazon.com/ 5 - Practice: + Created S3 buckets + Uploaded and downloaded data 25/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Attached EBS volume to EC2 + Verified volume operation 26/09/2025 26/09/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood the role of storage services in AWS. Differentiated between Amazon S3 and Amazon EBS. Successfully performed basic data management tasks on Amazon S3. Gained practical experience using EBS volumes with EC2 instances. Learned how to choose appropriate storage services for different use cases. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.4-s3-onprem/",
	"title": "Prepare Environment Configuration",
	"tags": [],
	"description": "",
	"content": "AWS Metropolitano Workshop In this section, you will prepare the entire environment for the Metropolitano system running on AWS. This environment includes real-time data processing services, data storage, backend API, data analytics, and CI/CD pipeline.\nThe goal of this section is to build a complete architecture for operating the Metropolitano train system, including:\nReal-time event streaming — processing passenger count, train status, and ticket transactions Event-driven processing using Amazon SQS and SNS RDS for persistent data storage S3 as a data lake for logs, raw data, and analytical datasets EventBridge for orchestrating events across the system CloudWatch for system monitoring QuickSight for data visualization CodePipeline + CodeBuild + CodeDeploy for automated deployment Metropolitano Architecture Overview The architecture below represents the ticketing system, passenger flow, data streaming, and event processing pipeline:\nWhy Prepare the Environment? The Metropolitano system uses multiple AWS services. Preparing the environment ensures:\nService-to-service connectivity — API Gateway ⇄ Lambda ⇄ RDS ⇄ S3 ⇄ DynamoDB Real-time data pipelines using Kinesis, SQS, SNS, and EventBridge Network infrastructure (VPC) for internal communication without using public internet Analytics readiness using QuickSight and an S3 Data Lake Automated application deployment using CodePipeline What You Will Configure in This Section During the Prepare Environment phase, you will create and configure the following:\nVPC \u0026amp; Networking\nSubnets, Route Tables, Security Groups Private \u0026amp; public networking for EC2, RDS, and Lambda EC2 Instances\nSimulated backend or worker nodes IAM Roles for S3 / DynamoDB / Kinesis access Amazon RDS\nPrimary database for the Metropolitano system Stores ticketing, user, and historical data S3 Data Lake\nraw/ (raw data) analytics/ (processed/analytical data) app-assets/ (frontend static assets) Kinesis Data Stream\nCollects real-time passenger metrics and train status SQS + SNS\nHandles transaction queues Sends delay or incident notifications EventBridge\nRoutes events between API, Lambda, Payment, and Alerts CloudWatch\nLogs, Metrics, Alarms, Dashboards QuickSight\nProvides analytical dashboards for system monitoring CodeBuild + CodeDeploy + CodePipeline\nAutomates backend build → test → deployment After Completing This Section You will have a fully prepared AWS environment ready to deploy the entire Metropolitano application, ensuring:\nReliable data storage Real-time event processing Flexible API integration Automated CI/CD pipeline Powerful analytics system "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "Event 1 Event Name: GenAI and Data (Gen AI and Data Track – AWS Summit)\nDate \u0026amp; Time: 09:00 – 17:30, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nShort Description:\nThe event was a full-day track focusing on Generative AI and data strategy on AWS. The sessions covered building a unified data platform for AI and analytics (data ingestion from multiple sources, cost-optimized storage, processing with AWS Glue, EMR, and Redshift, and data governance), AWS’s GenAI adoption roadmap with Amazon Bedrock and Amazon Q, the AI-driven software development lifecycle (AI-DLC), comprehensive security practices for GenAI applications, and the growing role of AI Agents in improving organizational productivity.\nOutcomes / Value Gained:\n• Strengthened understanding of GenAI adoption strategies on AWS, emphasizing small experiments and gradual scaling aligned with business objectives.\n• Gained a clear view of the core components of a unified data platform and the importance of governance and access control alongside storage and processing.\n• Acquired practical ideas for applying AI-DLC to improve development productivity, such as documentation support, test case generation, and code improvement suggestions.\n• Increased awareness of multi-layer GenAI security, including infrastructure, model access control, and application-layer protection.\nEvent 2 Event Name: GenAI-powered App-DB Modernization Workshop\nDate \u0026amp; Time: 09:00, August 13, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nShort Description:\nThe event was an in-depth workshop on application and database modernization, focusing on the transition from legacy architectures to modern approaches such as microservices and event-driven architecture. Key topics included Domain-Driven Design (DDD) techniques such as event storming, bounded contexts, and context mapping; comparisons of integration patterns (pub/sub, point-to-point, and streaming); and an analysis of how compute models evolve from EC2 to containers and serverless. The workshop also highlighted the use of AI tools, particularly Amazon Q Developer and transformation agents, to support and accelerate modernization efforts.\nOutcomes / Value Gained:\n• Developed a business-first mindset for system modernization and improved the ability to build a shared ubiquitous language among stakeholders.\n• Learned how to define clear service boundaries using bounded contexts and apply event-driven communication to reduce coupling between system components.\n• Gained a better understanding of the criteria for choosing between virtual machines, containers, and serverless options (functions and container-based).\n• Identified practical application paths such as organizing event storming sessions, refactoring services based on bounded contexts, experimenting with asynchronous messaging, and using Amazon Q Developer to enhance development productivity.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "🔹 Objectives – Week 4 Understand networking concepts in AWS and the role of Amazon VPC. Learn the basic components of a VPC: Subnets, Route Tables, Internet Gateway. Practice creating a simple VPC architecture and launching EC2 inside the VPC. 🔹 Tasks performed in Week 4 (EN) Day Task Start date Completion date References 2 Studied VPC concepts, CIDR, and Private/Public Networks 29/09/2025 29/09/2025 AWS Docs 3 Learned about Subnets, Route Tables, and Internet Gateways 30/09/2025 30/09/2025 AWS Docs 4 Hands-on: Created a VPC and a Public Subnet 01/10/2025 01/10/2025 FCJ 5 Attached an Internet Gateway and configured Route Tables 02/10/2025 03/10/2025 FCJ 6 Launched an EC2 instance within the VPC and tested connectivity 03/10/2025 03/10/2025 FCJ 🔹 Results Achieved – Week 4 (EN) Gained understanding of Amazon VPC and its importance in AWS architecture. Learned how to design a VPC using proper CIDR blocks. Successfully created: VPC Public Subnet Internet Gateway Route Table Launched an EC2 instance inside the VPC with successful Internet access. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Set up the project called Metropolitano Overview The Metropolitano AWS Architecture provides a secure, scalable, and highly available cloud environment for running transportation management workloads. This system leverages multiple AWS services to ensure high performance, strong security controls, fast content delivery, operational monitoring, and continuous deployment. In this workshop, you will learn how the Metropolitano platform is structured on AWS and how different components integrate to deliver a seamless experience for both end-users and internal operators.\nThroughout the lab, you will explore how the system uses core AWS components such as CloudFront, S3, Route 53, EC2, RDS, EventBridge, SQS, SNS, Kinesis, and CodePipeline.\nContent Workshop overview Prerequiste Create a secret manager Prepare enviroment Clean up "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "🔹 Objectives – Week 5 Understand AWS security and access management concepts. Learn IAM components: Users, Groups, Roles, and Policies. Apply the Principle of Least Privilege when assigning permissions. 🔹 Tasks performed in Week 5 Day Task Start date Completion date References 2 Studied an overview of IAM and the AWS security model 06/10/2025 06/10/2025 AWS Docs 3 Created IAM users and assigned basic permissions 07/10/2025 07/10/2025 FCJ 4 Learned about IAM groups and managed policies 08/10/2025 08/10/2025 FCJ 5 Hands-on: Assigned permissions using groups and policies 09/10/2025 09/10/2025 FCJ 6 Studied IAM roles and their usage with EC2 10/10/2025 10/10/2025 AWS Docs 🔹 Results Achieved – Week 5 Understood the importance of IAM in AWS security architecture. Learned how to: Create IAM Users and Groups Attach appropriate policies Manage access based on roles and responsibilities Applied security best practices: Avoid using the root account for daily tasks Grant only necessary permissions Gained understanding of how EC2 uses IAM Roles to securely access AWS services. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/5-workshop/5.5-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Clean Up Resources 1. Networking \u0026amp; VPC Clean Up 1.1 Delete Route 53 Hosted Zone \u0026amp; Resolver Rules Navigate to Route 53 → Hosted Zones.\nDelete the Private Hosted Zone s3.us-east-1.amazonaws.com.\nGo to Route 53 Resolver → Rules.\nDisassociate the rule myS3Rule from VPC Onprem, then delete it.\n1.2 Delete VPC Resources Steps:\nGo to EC2 → Network Interfaces\nDelete ENIs created for VPC Endpoints, Resolver Endpoints, or EC2 instances after termination. Go to VPC console:\nDelete Security Groups created for the lab. Delete Subnets (ensure no ENIs or route tables attached). Delete Route Tables. Delete Internet Gateway (detach first). Delete NAT Gateway (if created). Finally, delete the VPC metropolitano. 2. EC2 Clean Up Go to EC2 → Instances. Select instance metropolitano-version-1 and Terminate. Delete: Elastic IPs (if allocated) Key pair myKey.pm (local file optional) Security group public-web-sg 3. RDS Clean Up Go to RDS Console → Databases. Select SQL Server instance and choose Delete. Options: Disable final snapshot (optional) Delete the DB Subnet Group private-db-metropolitano. Delete the security group private-db-sg if unused. 4. S3 Clean Up Go to S3 Console. Empty the bucket metropolitano-2025. Click Delete bucket. 5. CloudFront Clean Up Go to CloudFront → Distributions. Select the distribution. Choose Disable → Wait for status “Disabled”. Click Delete. 6. Kinesis Data Stream Clean Up Go to Kinesis Console → Data Streams. Select metropolitano-stream. Click Delete. 7. EventBridge Clean Up Go to EventBridge → Rules. Select metropolitano-event-rule. Click Delete. If event buses or targets were created manually, delete them too. 8. SQS Queue Clean Up Go to SQS Console → Queues. Select metropolitano-queue. Click Delete. 9. SNS Clean Up Go to SNS Console → Topics. Select metropolitano-alerts. Delete subscriptions (email/SMS). Delete topic. 10. CloudWatch Clean Up 10.1 Delete Alarms CloudWatch → Alarms → Select alarms → Delete. 10.2 Delete Logs CloudWatch → Log Groups Delete: EC2 log groups Lambda log groups (if any) VPC Flow Logs (if created) 10.3 Delete Metrics (optional) Metrics automatically expire; no action required.\n11. QuickSight Clean Up If enabled, QuickSight can generate monthly cost.\nSteps:\nGo to QuickSight Console → Manage QuickSight.\nDelete:\nDatasets SPICE datasets Dashboards Analyses If not needed → Unsubscribe QuickSight account.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services (AWS) from September 08 to December 24, I had the opportunity to experience a professional working environment and apply academic knowledge to real-world tasks. This period allowed me to gain practical experience while improving both technical and soft skills.\nI participated in the Metro project, where I worked on assigned tasks related to programming, requirement analysis, technical documentation, and reporting. Through these activities, I gradually improved my ability to work independently, understand project requirements, and adapt to the team’s workflow.\nRegarding work attitude, I consistently aimed to complete tasks on time, comply with company regulations, and actively communicate with mentors and teammates whenever challenges arose. I believe these efforts helped me contribute positively to the project.\nBased on my overall internship experience, I present my self-evaluation as follows:\nNo. Criteria Description Good Fair Average 1 Technical Knowledge \u0026amp; Skills Understanding professional knowledge, applying it to tasks, work quality ✅ ☐ ☐ 2 Learning Ability Ability to acquire and adapt to new knowledge ☐ ✅ ☐ 3 Proactiveness Taking initiative and handling tasks independently ✅ ☐ ☐ 4 Responsibility Task completion, meeting deadlines, quality assurance ✅ ☐ ☐ 5 Discipline Compliance with rules, schedules, and workflows ☐ ✅ ☐ 6 Willingness to Improve Openness to feedback and self-improvement ☐ ✅ ☐ 7 Communication Skills Work presentation and professional communication ☐ ✅ ☐ 8 Teamwork Collaboration and participation in team activities ✅ ☐ ☐ 9 Professional Behavior Respectful attitude toward colleagues and workplace ✅ ☐ ☐ 10 Problem-Solving Skills Identifying issues and proposing appropriate solutions ☐ ✅ ☐ 11 Contribution Level of contribution to the project and team ✅ ☐ ☐ 12 Overall Evaluation General assessment of the internship experience ✅ ☐ ☐ Areas for Improvement Improving self-discipline and maintaining strict compliance with workplace regulations Enhancing problem-solving skills through deeper analysis and post-task reflection Developing clearer and more confident communication in daily interactions and work discussions "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Practice deploying a basic application on AWS. Understand the workflow of deploying applications on EC2. Get familiar with configuring Security Groups and service connectivity. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Review EC2 concepts, Security Groups, and Elastic IP. 13/10/2025 13/10/2025 AWS Docs 3 Set up the environment on EC2 (Linux OS and basic packages). 14/10/2025 14/10/2025 AWS Docs 4 Deploy a simple demo web application on EC2. 15/10/2025 15/10/2025 — 5 Configure Security Groups to allow HTTP/HTTPS and SSH access. 16/10/2025 16/10/2025 AWS Docs 6 Test connectivity, troubleshoot issues, and document encountered problems. 17/10/2025 17/10/2025 — Week 6 Outcomes: Successfully deployed a basic web application on an EC2 instance. Gained hands-on experience from EC2 initialization to application execution. Learned how to configure Security Groups for network access control. Encountered initial issues related to port configuration and firewall rules, which required multiple adjustments. Realized the importance of carefully reviewing security settings before deployment. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "General Feedback 1. Working Environment\nDuring my internship, I found the working environment to be friendly and open. Members of the FCJ program were always willing to support and discuss work-related issues when I faced difficulties. The workspace was organized and comfortable, which helped me stay focused. However, having more informal activities or team bonding sessions could further strengthen connections among team members.\n2. Support from Mentor and Program Team\nMy mentor provided clear guidance and patiently explained topics whenever I had questions. The admin team supported interns well in terms of documentation, schedules, and procedures. I especially appreciated the mentor’s approach of encouraging me to explore solutions independently before giving direct answers, which helped me learn more effectively.\n3. Relevance Between Internship Tasks and Major\nThe tasks assigned during the internship were aligned with my Software Engineering background while also introducing new areas that I had not experienced before. This balance allowed me to reinforce my foundational knowledge and gain practical exposure to real-world systems and workflows.\n4. Learning Opportunities and Skill Development\nThroughout the internship, I developed various skills such as using project management tools, collaborating within a team, and communicating in a professional environment. Additionally, the mentor’s real-world insights helped me better understand potential career paths and workplace expectations.\n5. Company Culture and Team Spirit\nThe company culture was positive and supportive. Team members treated each other with respect and maintained a professional yet comfortable working atmosphere. During high-pressure tasks, everyone worked together to meet deadlines, which made me feel included as part of the team despite being an intern.\n6. Internship Policies and Benefits\nThe internship program provided financial support and allowed flexibility when necessary. Interns also had opportunities to attend internal training sessions, which significantly enhanced the overall learning experience.\nAdditional Questions What did you enjoy the most during your internship?\nThe aspect I enjoyed the most was the open and supportive working environment. Mentors consistently encouraged questions and allowed interns to try solving problems independently. Being exposed to professional workflows and internal knowledge-sharing sessions helped me gain valuable practical experience.\nWhat aspects could be improved for future interns?\nFrom my perspective, the program could be improved by:\nProviding a clearer onboarding plan with weekly milestones and expected outcomes. Organizing short topic-focused sharing sessions to standardize knowledge among interns. Creating more opportunities for cross-team interaction or team bonding activities. Would you recommend this internship program to others? Why?\nYes, I would recommend this program. It offers a supportive environment, practical tasks, and strong mentorship. More importantly, it helps interns understand how to work professionally, not just complete assigned tasks.\nSuggestions and Expectations Suggestions for improving the internship experience\nDeveloping a centralized intern handbook or knowledge base (e.g., Notion) containing guidelines, FAQs, and reporting templates. Holding short weekly sync meetings between interns and mentors/admins to quickly address obstacles. Organizing mini demo or presentation sessions at the end of each phase to receive early feedback. Future Participation\nIf given the opportunity, I would be interested in continuing with the program in a more advanced intern role or supporting future interns, as the experience has been highly beneficial for my personal and professional growth.\nAdditional Comments\nI sincerely appreciate the support from the FCJ team, mentors, and program administrators throughout my internship. I hope the program continues to improve and provides even better opportunities for future interns.\n"
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn about monitoring and logging on AWS. Get familiar with Amazon CloudWatch. Monitor the status of deployed resources and applications. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Study CloudWatch overview (Metrics and Logs). 20/10/2025 20/10/2025 AWS Docs 3 Monitor basic EC2 metrics such as CPU utilization and network traffic. 21/10/2025 21/10/2025 AWS Docs 4 Configure CloudWatch Logs for applications running on EC2. 22/10/2025 22/10/2025 — 5 Create simple alarms to monitor resource usage. 23/10/2025 23/10/2025 AWS Docs 6 Review logs, evaluate results, and note difficulties when working with CloudWatch. 24/10/2025 24/10/2025 — Week 7 Outcomes: Understood the concept of monitoring and logging in cloud environments. Learned how to use CloudWatch to track EC2 metrics. Successfully configured CloudWatch Logs for a deployed application. Faced initial difficulties configuring log agents and IAM permissions. Realized that proper monitoring helps detect issues and resource overload early. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Get familiar with CI/CD concepts. Understand basic build and deployment automation. Learn AWS services that support CI/CD pipelines. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Study CI/CD concepts and basic pipeline workflow. 27/10/2025 27/10/2025 AWS Docs 3 Review AWS CodePipeline and CodeBuild overview. 28/10/2025 28/10/2025 AWS Docs 4 Practice creating a simple pipeline (source → build). 29/10/2025 29/10/2025 — 5 Integrate the pipeline with EC2 to deploy a demo application. 30/10/2025 30/10/2025 — 6 Test the pipeline and document issues and unoptimized steps. 31/10/2025 31/10/2025 — Week 8 Outcomes: Gained an understanding of CI/CD concepts and their role in software development. Learned the main components of a CI/CD pipeline. Successfully created a simple pipeline to automate application builds. Faced difficulties configuring IAM permissions for pipeline services. Realized that automation reduces manual errors but requires careful configuration. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Understand the concept of Infrastructure as Code (IaC). Get familiar with AWS CloudFormation. Deploy basic infrastructure using templates. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Learn IaC concepts and benefits for infrastructure management. 03/11/2025 03/11/2025 AWS Docs 3 Study CloudFormation template structure using YAML format. 04/11/2025 04/11/2025 AWS Docs 4 Write a simple template to create EC2 and a Security Group. 05/11/2025 17/09/2025 — 5 Deploy a CloudFormation stack and monitor resource creation. 06/11/2025 05/11/2025 — 6 Modify the template, fix errors, and document lessons learned. 07/11/2025 07/11/2025 — Week 9 Outcomes: Understood Infrastructure as Code and its importance. Learned the basic structure of CloudFormation templates. Successfully created and deployed a simple CloudFormation stack. Encountered errors related to incorrect resource references and had to revise the template multiple times. Realized that IaC improves consistency but requires careful syntax validation. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Understand how AWS services are charged. Learn tools for cost monitoring and management. Adjust resources to avoid unnecessary costs. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Study cost models for used AWS services such as EC2 and S3. 10/11/2025 10/11/2025 AWS Docs 3 Explore AWS Cost Explorer and Billing Dashboard. 11/11/2025 11/11/2025 AWS Console 4 Review running resources and identify unnecessary ones. 12/11/2025 12/11/2025 — 5 Stop or remove unused resources to reduce costs. 13/11/2025 13/11/2025 — 6 Document lessons learned and notes on cloud cost management. 14/11/2025 14/119/2025 — Week 10 Outcomes: Understood how AWS services generate costs over usage time. Learned to use Cost Explorer to monitor and estimate expenses. Realized that unused resources can still incur charges if left running. Began forming a habit of checking costs regularly. Recognized that cost management is as important as technical implementation. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Review AWS knowledge learned during the internship. Finalize the deployed demo system. Check, troubleshoot, and improve the system configuration. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Review EC2, VPC, Security Groups, and IAM concepts. 24/11/2025 24/11/2025 AWS Docs 3 Recheck the deployed system from previous weeks. 25/11/2025 25/11/2025 — 4 Fix some misconfigured settings such as security rules and ports. 26/11/2025 26/11/2025 — 5 Review logs and alarms to ensure system stability. 27/11/2025 27/11/2025 CloudWatch 6 Summarize encountered issues and lessons learned. 28/11/2025 28/11/2025 — Week 11 Outcomes: Reinforced AWS knowledge gained throughout the internship. Improved system stability through configuration refinement. Identified and fixed initial misconfigurations. Realized the importance of post-deployment review and monitoring. Prepared for final evaluation and overall reflection. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Summarize the entire internship process. Evaluate the knowledge and skills gained. Identify strengths, weaknesses, and personal lessons learned. Tasks Conducted This Week: Day Task Start Date Completion Date Reference 2 Summarize all tasks and activities completed during the 12-week internship. 01/12/2025 01/12/2025 — 3 Evaluate understanding of AWS services learned throughout the internship. 02/12/2025 02/12/2025 — 4 Review encountered issues and the methods used to resolve them. 03/12/2025 03/12/2025 — 5 Write the final internship report and prepare the self-assessment. 04/12//2025 04/12/2025 — 6 Finalize the worklog and submit the internship report. 05/12//2025 05/12/2025 — Week 12 Outcomes: Successfully summarized all knowledge and skills gained during the internship. Developed a clearer understanding of working with AWS from basic concepts to system management. Recognized personal limitations in advanced and complex automation scenarios. Learned the importance of careful configuration, especially regarding security and cost management. Established clearer direction for future learning and development in cloud technologies. Lessons Learned: Cloud learning requires a combination of theory and continuous hands-on practice. Small configuration mistakes can lead to significant issues if not carefully reviewed. Monitoring, logging, and cost management are essential aspects of cloud operations. Internships play a critical role in developing system thinking and real-world working habits. "
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://nguyenthinhauyen.github.io/FJC-Report/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]